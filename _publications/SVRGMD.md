---
title: "Variance Reduction on Adaptive Stochastic Mirror Descent"
collection: publications
permalink: /publications/SVRGMD
excerpt: 
date: 2020-10-30
venue: The 34th Conference on Neural Information Processing Systems (NeurIPS 2020) Workshop & OPT
paperurl:
citation: <b>Wenjie Li</b>, Zhanyu Wang, Yichen Zhang, Guang Cheng
---
[[OPT2020 Workshop paper](https://opt-ml.org/papers/2020/paper_19.pdf)] [[slides](https://williamlwj.github.io/About/files/slides/SVRGMD_paper_slides.pdf)]

We study the application of the variance reduction technique on general adaptive stochastic mirror descent algorithms in nonsmooth nonconvex optimization problems. We prove that variance reduction helps to reduce the gradient complexity of most general stochastic mirror descent algorithms, so it works well with time-varying steps sizes and adaptive optimization algorithms such as AdaGrad. We check the validity of our claims using experiments in deep learning.
